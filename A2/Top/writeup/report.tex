\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs,floatrow}
\graphicspath{ {../output/}}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
}

\title{COL380 - Parallel and Distributed Programming : Assignment - 2}
\author{Harish Kumar Yadav - 2018CS10337 \\ Param Khakhar - 2018CS10362}
\date{05 April 2021}

\begin{document}

\maketitle

\section *{Introduction}

The following is a writeup for the Assignment-2, Parallel and Distributed Programming (COL380). There are sections corresponding to each of the four strategies implemented, which also contains the answers to the questions.

\section*{Strategy-1 : Using Parallel For}

% \subsection*{Description:}

% \subsection*{Correctness:}

% \section*{Strategy-2 : Using Sections}

% \subsection*{Description:}

% \subsection*{Correctness:}

% \section*{Strategy-3 : Using both Parallel For and Sections}

% \subsection*{Description:}

% \subsection*{Correctness:}

% \section*{Strategy-4 : Distributed Version}

% \subsection*{Description:}

% \subsection*{Correctness:}



%   Two different parallelized versions of the summation are implemented. 
%   \begin{itemize}
%     \item In the first approach, nested parallelism is used, and a local variable corresponding to each thread stores the total sum for that thread, and finally the local sums from the all the threads are added in the global variable sum.
%     \item The second approach is an implementation of the tree based approach as discussed in the second lecture. In this approach, pairs of elements are added and the size of the array reduces by half at each stage. The height, of the tree so formed would be \textbf{O(logN)}.
%   \end{itemize}

% \section*{Task -2 Performance Analysis}

%   \subsection*{2.1 First Approach}

%   The following execution times for the entire programs and the parallelized parts was observed for the first approach on various threads and inputs sizes.
%   \begin{table}[H]
%     \begin{tabular}{lllll} \toprule
%         \textbf{Limit$|$Threads}& \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8}\\\midrule
%         1e03 & 0.041 & 0.290 & 0.197 & 8.811\\
%         1e05 & 3.813 & 3.636 & 2.338 & 5.791\\
%         1e07 & 122.026 & 95.856 & 87.911 & 85.502\\
%         \bottomrule
%       \end{tabular}
%     \caption{The Time Values are in milliseconds}\label{Tab1}
%   \end{table} 

%   The following \emph{speedup} and \emph{efficiency} values are observed for different number of threads and limits for summation for the first approach. \\

%   \begin{table}[H]
%     \begin{tabular}{llllll} \toprule
%         \textbf{Size} &\textbf{p} & \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8}\\\midrule
%         1e03 & S & 1.0 & 0.1415 & 0.2086 & 0.0047\\
%              & E & 1.0 & 0.0707 & 0.0521 & 0.0005\\
%         1e05 & S & 1.0 & 1.0487 & 1.6310 & 0.6584\\
%              & E & 1.0 & 0.5243 & 0.4078 & 0.0823\\
%         1e07 & S & 1.0 & 1.273 & 1.3881 & 1.4271 \\
%              & E & 1.0 & 0.6365 & 0.347 & 0.1789
%         \\\bottomrule
%       \end{tabular}
%     \caption{S = Speedup, E = Efficiency}\label{Tab1}
%   \end{table} 

%   \subsection*{2.2 Second Approach}
  
%     The following execution times for the entire programs and the parallelized parts was observed for the second approach on various threads and inputs sizes.
%     \begin{table}[H]
%       \begin{tabular}{lllll} \toprule
%           \textbf{Limit$|$Threads}& \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8}\\\midrule
%           1e03 & 0.041 & 0.134 & 0.123 & 0.110\\
%           1e05 & 3.813 & 4.648 & 3.951 & 2.56\\
%           1e07 & 122.026 & 125.327 & 123.177 & 112.395\\
%           \bottomrule
%         \end{tabular}
%       \caption{The Time Values are in milliseconds}\label{Tab1}
%     \end{table} 

%     The following \emph{speedup} and \emph{efficiency} values are observed for different number of threads and limits for summation for the first approach. \\

%     \begin{table}[H]
%       \begin{tabular}{llllll} \toprule
%           \textbf{Size} &\textbf{p} & \textbf{1} & \textbf{2} & \textbf{4} & \textbf{8}\\\midrule
%           1e03 & S & 1.0 & 0.3068 & 0.3324 & 0.3715\\
%               & E & 1.0 & 0.1534 & 0.0831 & 0.0464\\
%           1e05 & S & 1.0 & 0.8204 & 0.9652 & 1.4897\\
%               & E & 1.0 & 0.4102 & 0.2413 & 0.1862\\
%           1e07 & S & 1.0 & 0.9737 & 0.9907 & 1.0857 \\
%               & E & 1.0 & 0.4868 & 0.2477 & 0.1357
%           \\\bottomrule
%         \end{tabular}
%       \caption{S = Speedup, E = Efficiency}\label{Tab1}
%     \end{table} 

%     \section*{Task -3 Remarks}

%     \begin{itemize}
%       \item From the above observations, we can conclude that \textbf{Amdahl's Law} is approximately followed, as the \textbf{efficiency values} are \textbf{decreasing consistently} for both the approaches, which implies that the added advantage on increasing the threads is decreasing and ultimately the overheads for parallelizing the code would be more as compared to the advantage on the computation time as is the case for 8 threads in the first approach for the input size of 1e03, and 1e05.
%       \item During the experimentation, significant variations were present in the above data, and the values mentioned above are the ones which were occurring most frequently during multiple executions.
%       \item The \emph{Speedup} values are increasing mostly along a row as well as along a column.
%       \item The \emph{Efficiency} values are decreasing mostly along a row but increasing along a column.
%       \item The \emph{Serial} approach performs reasonably well, which can be attributed to the simplicity of the task as well as caching of the entries by the memory.
%     \end{itemize}


%   \subsection*{Task - 1.a: Batch Gradient Descent}
%   The theta\_parameters are initialized as all zeros. The learning rate of 0.025 is able to converge after 382 iterations. The final value of the parameters is found to be $\theta_{0}$ = 0.99657 and $\theta_{1}$ = 0.00136. The stopping criteria used is when $|J(\theta\textsuperscript{t+1}) - J(\theta\textsuperscript{t})| \leq 10\textsuperscript{-10}$.

%   \subsection*{Task -1.b: Plotting the Data Points}

%   \includegraphics[width=13cm]{../output/q1/q1b}

%   The graph is plotted on normalized data as the values of the target variables are very close to each other. The acidity values have been normalized.

%   \subsection*{Task - 1.c: Meshgrid animation}

%   The file q1c.mp4 is available in the output directory, which shows the progress. The snapshot is as follows:

%   \includegraphics[width=13cm]{../output/q1/q1c}

%   \subsection*{Task - 1.d: Contour animation}

%   The file q1d.mp4 is available in the output directory, which shows the progress. The snapshot is as follows:

%   \includegraphics[width=13cm]{../output/q1/q1d}

%   \subsection*{Task - 1.e: Contour Animation Comparison}

%   As evident in the video, the marker with learning rate = 0.001 is making very slow progress, whereas the markers with learning rate 0.025 and 0.1 have already converged. The video isn't completely save due to lack of efficient computational resources. \\

%   The following code was referred while plotting the visualization of contours and 3d mesh, https://xavierbourretsicotte.github.io/animation\_ridge.html

%   \section* {Task - 2: Stochastic Gradient Descent}

%   \subsection* {Task - 2.a: Sampling}
%   Numpy's inbuilt functions were used to sample values from a normal distribution.

%   \subsection*{Task - 2.b: Training}
%   Given the batch size, the data is shuffled randomly, and then divided into batches of size b. During each iteration, cost is computed for every batch, followed by computation of gradients, and updating the theta parameters. The convergence criteria is when for all the batches, the value of the cost in an iteration, and a consecutive iteration, becomes lesser than a particular threshold which is set as $10\textsuperscript{-8}$.

%   \subsection*{Task - 2.c: Training Statistics}
%   The following table indicates the statistics while training the model for different batch sizes, each with a threshold equal to $10\textsuperscript{-8}$ and a learning rate of 0.001:

%   \begin{table}[h!]
%     \begin{center}
%       %\label{tab:table1}
%       \begin{tabular}{|c|c|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
%       \hline
%       \textbf{Batch Size} & \textbf{Epochs} & \textbf{Time in s} & \textbf{Final Parameters}\\
%         %$\alpha$ & $\beta$ & $\gamma$ \\
%         \hline  
%         $10\textsuperscript{6}$ & 20284 & 523s & [2.989,1.002,1.999]\\
%         $10\textsuperscript{4}$ & 277 & 5.51s & [2.997,1.000,2.000]\\
%         $10\textsuperscript{2}$ & 7 & 1.23s & [3.000,0.9954,2.0013]\\
%         $1$ & 2 & 38.4s & [3.007,0.9896,1.9933]\\
%         \hline
%       \end{tabular}
%     \end{center}
%   \end{table}

%   The differences in different batch sizes is clearly visible, in terms of the number of epochs, and the time required in s for convergence. Increasing the batch size, increasing the batch size decreases the time in which the solution converges, upto a certain extent, however for batch size = 1, the time required for convergence, is higher as compared to batch size = 100, and batch size = 10000. \\

%   The final value of the parameters in each and every case is very close to the original parameters, which are [3,1,2]. The following table represents the \textbf{Mean Squared Error} on the Test Set for different batch sizes and the original hypothesis. \\ \\ \\ \\ \\

%   \begin{table}[h!]
%     \begin{center}
%       %\label{tab:table1}
%       \begin{tabular}{|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
%       \hline
%       \textbf{Batch Size} & \textbf{MSE}\\
%         %$\alpha$ & $\beta$ & $\gamma$ \\
%         \hline
%         Original Hypothesis & 0.9826\\  
%         $10\textsuperscript{6}$ & 0.9837\\
%         $10\textsuperscript{4}$ & 0.9832\\
%         $10\textsuperscript{2}$ & 0.9833\\
%         $1$ & 1.1138\\
%         \hline
%       \end{tabular}
%     \end{center}
%   \end{table}

%   The error for the original hypothesis is the least among all the different learnt parameters for different batches. however, the difference is less, and thereby implying that the learning algorithm's performance is fairly good in learning the parameters.

%   \subsection*{Task - 2.d}

%     \includegraphics[width=13cm]{../output/q2/q2d}

%   The path taken when the batch size is 1 is very oscillatory. Also, this oscillatory nature of the path decreases as the batch size increases. It makes intuitive sense as increasing the number of examples for calculating the gradient would result in more appropriate direction of the gradient which won't be the case when considering lesser number of samples.
% \section*{Task - 3: Logistic Regression}

%   The value of optimal parameters obtained using Newton's Method are: $\theta_{0}$ = 0.46722676, $\theta_{1}$ = 2.57071759, $\theta_{2}$ = -2.7955926. The process got completed in 7 steps, and the threshold for convergence is 10\textsuperscript{-6}. The decision boundary corresponding to the parameters is:

%   \includegraphics[width=13cm]{../output/q3/q3b}

% \section*{Task - 4: Gaussian Discriminant Analysis}

%   \subsection*{Task - 4.a : Linear Discriminant Analysis}
%   The analytically computed values of the parameters are as follows:
%   \begin{itemize}
%     \item $\Phi$ = 0.5
%     \item $\mu_{0}$ = $\begin{bmatrix}
%                       0.75529433 & -0.68509431
%                       \end{bmatrix}$
%     \item $\mu_{1}$ = $\begin{bmatrix}
%                       -0.75529433 & 0.68509431
%                       \end{bmatrix}$
%     \item $\Sigma$ =  $\begin{bmatrix}
%                       0.42953048 & -0.02247228 \\
%                       -0.02247228 & 0.53064579
%                       \end{bmatrix}$
%   \end{itemize}

%   The training data given along with the linear decision boundary is as follows: \\

%   \includegraphics[width=13cm]{../output/q4/q4c}

%   \subsection*{Task - 4.d : Quadratic Discriminant Analysis}

%   The analytically computed values of the parameters are as follows:
%   \begin{itemize}
%     \item $\Phi$ = 0.5
%     \item $\mu_{0}$ = $\begin{bmatrix}
%                       0.75529433 & -0.68509431
%                       \end{bmatrix}$
%     \item $\mu_{1}$ = $\begin{bmatrix}
%                       -0.75529433 & 0.68509431
%                       \end{bmatrix}$
%     \item $\Sigma_{0}$ =  $\begin{bmatrix}
%                       0.47747117 & 0.1099206 \\
%                       0.1099206 & 0.68509431
%                       \end{bmatrix}$
%     \item $\Sigma_{1}$ =  $\begin{bmatrix}
%                         0.38158978 & -0.15486516 \\
%                         -0.15486516 & 0.64773717
%                         \end{bmatrix}$
%     \end{itemize}

%     The decision boundary for both linear and quadratic discriminant analysis is as follows: 

%     \includegraphics[width=13cm]{../output/q4/q4e}

%     The quadratic decision boundary is better as compared to the linear decision boundary, as certain points would have been misclassified by the linear boundary, but they are rightly classified by the quadratic decision boundary.


\end{document}